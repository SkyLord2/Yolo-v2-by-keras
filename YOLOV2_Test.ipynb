{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"YOLOV2_Test.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1vq8yD1iyqoKRb813iDfdgINRRiOsHrNK","authorship_tag":"ABX9TyOiyUOgcuw1ErdMrpq94NfV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kLPXtMlmehUM","executionInfo":{"status":"ok","timestamp":1608201402980,"user_tz":-480,"elapsed":885,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}}},"source":["import sys\n","sys.path.append(\"./drive/My Drive/PapperReproduce/YOLOv2\")\n","sys.path.append(\"./drive/My Drive/PapperReproduce/YOLOv2/YOLOv2\")\n","sys.path.append(\"./drive/My Drive/PapperReproduce/YOLOv2/utils\")\n","sys.path.append(\"./drive/My Drive/PapperReproduce/YOLOv2/data\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"YmdxHRSifVLE","executionInfo":{"status":"ok","timestamp":1608201405477,"user_tz":-480,"elapsed":2422,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}}},"source":["# from YOLOv1 import YOLOv1Net\n","# from YOLOv1_beta import YOLOv1Net\n","import time\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import numpy as np\n","import config as cfg\n","import matplotlib.pyplot as plt\n","from pascal_voc import pascal_voc\n","from timer import Timer\n","import config as cfg\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import tensorflow.keras.backend as K"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"P54fmuc6fje0"},"source":["#!cd drive/MyDrive/PapperReproduce/YOLOv2/data/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJloK10ci6iD"},"source":["#!tar xf drive/MyDrive/PapperReproduce/YOLOv2/data/VOCtrainval_06-Nov-2007.tar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xL2grZ_TkF8o"},"source":["#!mv VOCdevkit drive/MyDrive/PapperReproduce/YOLOv2/data/pascal_voc/."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGTtzcwYF7SP"},"source":["# 启用tpu\n","# %tensorflow_version 2.x\n","# print(\"Tensorflow version \" + tf.__version__)\n","# try:\n","#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","# except ValueError:\n","#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","# tf.config.experimental_connect_to_cluster(tpu)\n","# tf.tpu.experimental.initialize_tpu_system(tpu)\n","# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCWPUO9Wk0ya","executionInfo":{"status":"ok","timestamp":1608201408429,"user_tz":-480,"elapsed":877,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}},"outputId":"236d883a-2ac0-459a-f970-fdf9c756a6e3"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Thu Dec 17 10:36:50 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o7ckGuPDVS7R","executionInfo":{"status":"ok","timestamp":1608201414379,"user_tz":-480,"elapsed":851,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}},"outputId":"f6acd77f-a639-4957-ca48-95e7c71f12cb"},"source":["pascal = pascal_voc('train')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["file path= /content/drive/My Drive/PapperReproduce/YOLOv2/utils/pascal_voc.py\n","Loading gt_labels from: /content/drive/MyDrive/PapperReproduce/YOLOv2/data/pascal_voc/cache/pascal_train_gt_labels.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rm_HszyhKFpv"},"source":["images, labels, gt_boxes, response_anchors = pascal.get()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-9pDhgEKRSB"},"source":["for i in range(len(gt_boxes)):\r\n","    print(gt_boxes[i][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4gtbWmitvQw","executionInfo":{"status":"ok","timestamp":1607856704941,"user_tz":-480,"elapsed":813,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}},"outputId":"7f739d7a-9a72-4d68-8feb-3ee281f9c208"},"source":["np.any(labels[1] != 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"RneRvbazllkN","executionInfo":{"status":"ok","timestamp":1608215095483,"user_tz":-480,"elapsed":2434,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}}},"source":["# -*- coding: utf-8 -*-\n","# @Time : 2020/12/7 19:39\n","# @Author : cds\n","# @Site : https://github.com/SkyLord2?tab=repositories\n","# @Email: chengdongsheng@outlook.com\n","# @File : YOLOv22.py\n","# @Software: PyCharm\n","from tensorflow.keras.layers import Lambda, Conv2D, BatchNormalization, LeakyReLU, MaxPool2D, Input, GlobalAveragePooling2D, Softmax, concatenate\n","from tensorflow.keras.models import Model\n","\n","class YOLOv2:\n","    def __init__(self, alpha = 0.1, model_path = None, training = True):\n","        K.set_floatx('float32')  # 设置默认的数据类型\n","        self.alpha = alpha\n","        self.model_path = model_path\n","        self.batch_size = cfg.BATCH_SIZE\n","        self.epochs = cfg.EPOCHS\n","        self.learning_rate = cfg.LEARNING_RATE\n","        self.momentum = cfg.MOMENTUM\n","        self.lambda_noobj = cfg.NOOBJECT_SCALE\n","        self.lambda_coord = cfg.COORD_SCALE\n","        self.lambda_obj = cfg.OBJECT_SCALE\n","        self.lambda_class = cfg.CLASS_SCALE\n","        self.anchors = cfg.YOLO_ANCHORS\n","        self.num_anchor = len(self.anchors)\n","        self.classes = cfg.CLASSES\n","        self.num_classes = len(self.classes)\n","        self.input_size = cfg.IMAGE_SIZE\n","        self.conv_index = self.generateOffsetGrid([self.input_size // 32, self.input_size // 32], tf.float32)\n","        self.training = training\n","        self.model = self.build(self.input_size, self.num_classes, self.num_anchor, self.alpha)\n","        # 加载权重\n","        if(model_path is not None):\n","            self.model_load(model_path)\n","\n","\n","    def build(self, input_size, num_classe, num_anchor = 5, alpha = 0.1):\n","        \"\"\"\n","        构建DarkNet-19\n","        :param num_classes: 检测目标类别数量\n","        :param alpha: leaky relu 的激活系数\n","        :return model:  网络模型\n","        \"\"\"\n","        def PassThrough(x):\n","            return tf.compat.v1.space_to_depth(x, block_size=2)\n","        keras.backend.clear_session()\n","        # 输入尺寸随迭代变化\n","        input_image = Input(shape=(input_size, input_size, 3), dtype=\"float32\")\n","        # pad size: kernel_size//2\n","        # 使用BN层之后，卷积层没有偏置\n","        inter_tensor = Conv2D(filters=32, kernel_size=3, padding=\"same\", name=\"conv1\", use_bias=False)(input_image)\n","        inter_tensor = BatchNormalization(name=\"bn1\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = MaxPool2D(pool_size=(2, 2), strides=2)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=64, kernel_size=3, padding=\"same\", name=\"conv2\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn2\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = MaxPool2D(pool_size=(2, 2), strides=2)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=128, kernel_size=3, padding=\"same\", name=\"conv3\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn3\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=64, kernel_size=1, padding=\"same\", name=\"conv4\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn4\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=128, kernel_size=3, padding=\"same\", name=\"conv5\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn5\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = MaxPool2D(pool_size=(2, 2), strides=2)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=256, kernel_size=3, padding=\"same\", name=\"conv6\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn6\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=128, kernel_size=1, padding=\"same\", name=\"conv7\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn7\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=256, kernel_size=3, padding=\"same\", name=\"conv8\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn8\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = MaxPool2D(pool_size=(2, 2), strides=2)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=512, kernel_size=3, padding=\"same\", name=\"conv9\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn9\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=256, kernel_size=1, padding=\"same\", name=\"conv10\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn10\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=512, kernel_size=3, padding=\"same\", name=\"conv11\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn11\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=256, kernel_size=1, padding=\"same\", name=\"conv12\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn12\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=512, kernel_size=3, padding=\"same\", name=\"conv13\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn13\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        # passthrough layer 分支\n","        pass_through = inter_tensor\n","\n","        inter_tensor = MaxPool2D(pool_size=(2, 2), strides=2)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=1024, kernel_size=3, padding=\"same\", name=\"conv14\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn14\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=512, kernel_size=1, padding=\"same\", name=\"conv15\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn15\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=1024, kernel_size=3, padding=\"same\", name=\"conv16\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn16\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=512, kernel_size=1, padding=\"same\", name=\"conv17\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn17\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=1024, kernel_size=3, padding=\"same\", name=\"conv18\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn18\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=1024, kernel_size=3, padding=\"same\", name=\"conv19\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn19\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        inter_tensor = Conv2D(filters=1024, kernel_size=3, padding=\"same\", name=\"conv20\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn20\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","\n","        # passthrough layer\n","        pass_through = Conv2D(filters=64, kernel_size=3, padding=\"same\", name=\"pass_through_conv\", use_bias=False)(pass_through)\n","        pass_through = BatchNormalization(name=\"pass_through_bn\")(pass_through)\n","        pass_through = LeakyReLU(alpha)(pass_through)\n","        pass_through = Lambda(PassThrough)(pass_through)\n","\n","        inter_tensor = concatenate([pass_through, inter_tensor])\n","\n","        inter_tensor = Conv2D(filters=1024, kernel_size=3, padding=\"same\", name=\"conv21\", use_bias=False)(inter_tensor)\n","        inter_tensor = BatchNormalization(name=\"bn21\")(inter_tensor)\n","        inter_tensor = LeakyReLU(alpha)(inter_tensor)\n","        # 输出batchSize * 13 * 13 * (5 + 25)\n","        # 每个grid cell 分配5个anchor\n","        # 总共有20个类别\n","        yolo_out = Conv2D(filters=num_anchor*(num_classe + 5), kernel_size=1, padding=\"same\", name=\"conv22\", use_bias=False)(inter_tensor)\n","        model = None\n","        if(self.training):\n","            print(\"model training......\")\n","            # 这里是比较trick的地方，在网络的最后直接通过Lambda层来计算网络的损失，即网络的输出就是网络的损失\n","            gt_boxes_input = Input(shape=(None, 5), dtype=tf.float32)\n","            response_anchor_input = Input(shape=(input_size//32, input_size//32, self.num_anchor, 1), dtype=tf.float32)\n","            matching_true_boxes_input = Input(shape=(input_size//32, input_size//32, self.num_anchor, 5), dtype=tf.float32)\n","            output = Lambda(self.yolo_loss, output_shape=(1,), name=\"yolo_loss\", arguments={'anchors': self.anchors, 'num_classes': self.num_classes})([yolo_out, gt_boxes_input, response_anchor_input, matching_true_boxes_input])\n","            # 构建模型\n","            model = Model(inputs=[input_image, gt_boxes_input, response_anchor_input, matching_true_boxes_input], outputs=output)\n","        else:\n","            print(\"model predicting......\")\n","            model = Model(inputs=input_image, outputs=yolo_out, name=\"model_body\")\n","        return model\n","\n","    def compile_model(self):\n","        \"\"\"\n","        论文中的角动量为0.9，学习率为 1e-3\n","        :param model: 学习模型\n","        :return: None\n","        \"\"\"\n","        optimizer = keras.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum, clipnorm=1.)\n","        # 网络的损失已经在网络的最后一层进行计算，网络的输出已经是损失\n","        self.model.compile(optimizer = optimizer, loss = {\"yolo_loss\": lambda y_true, y_pred: y_pred}, metrics=['mse'])\n","\n","    # YOLO_ANCHORS = np.array(((0.57273, 0.677385), (1.87446, 2.06253), (3.33843, 5.47434),(7.88282, 3.52778), (9.77052, 9.16828)))\n","    # anchors_value = np.array([[1.08, 1.19], [3.42, 4.41], [6.63, 11.38], [9.42, 5.11], [16.62, 10.52]], dtype='float32')\n","\n","    def train(self, data, labels, learning_scheduler = None):\n","        \"\"\"\n","        :param data: 训练数据\n","        :param labels: 标签\n","        :return:\n","        \"\"\"\n","        if(learning_scheduler is None):\n","            def lr_scheduler(epoch):\n","                lr = 1e-5\n","                if (epoch <= 60):\n","                    lr = 1e-5\n","                elif (60 < epoch and epoch <= 90):\n","                    lr = 1e-6\n","                elif (90 < epoch and epoch <= 135):\n","                    lr = 1e-7\n","                return lr\n","            learning_scheduler = lr_scheduler\n","        lr_schedule = tf.keras.callbacks.LearningRateScheduler(learning_scheduler)\n","        history = self.model.fit(data, labels, batch_size=self.batch_size, epochs=self.epochs, callbacks=[lr_schedule])\n","        return history\n","    def train_generator(self, generator, data_size, callbacks=None):\n","        if(callbacks is None):\n","            def lr_scheduler(epoch):\n","                lr = 1e-3\n","                if (epoch <= 60):\n","                    lr = 1e-3\n","                elif (60 < epoch and epoch <= 90):\n","                    lr = 1e-4\n","                elif (90 < epoch and epoch <= 135):\n","                    lr = 1e-5\n","                return lr\n","            lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","            callbacks = [lr_schedule]\n","        self.model.fit_generator(generator(), steps_per_epoch=data_size//self.batch_size, epochs=self.epochs, callbacks=callbacks)\n","\n","    def model_summary(self):\n","        self.model.summary()\n","\n","    def model_save(self, path):\n","        \"\"\"\n","        保存模型\n","        :param path: 保存路径 yolo_v1_model.h5\n","        :return:\n","        \"\"\"\n","        self.model.save_weights(path)\n","\n","    def model_load(self, path):\n","        print(\"load model weights......\")\n","        self.model.load_weights(path)\n","\n","    def generateOffsetGrid(self, conv_dim, dtype):\n","        \"\"\"\n","        生成网格的偏执\n","        :param conv_dim: x,y方向的网格数量\n","        :param dtype: 数据类型\n","        :return: conv_index\n","        \"\"\"\n","        conv_height_index = tf.reshape(np.arange(start=0, stop=conv_dim[0]), (conv_dim[0],),\n","                                       name=\"generateOffsetGrid_reshape_conv_height_index\")\n","        conv_width_index = tf.reshape(np.arange(start=0, stop=conv_dim[1]), (conv_dim[1],),\n","                                      name=\"generateOffsetGrid_reshape_conv_width_index\")\n","\n","        conv_height_index = tf.tile(conv_height_index, [conv_dim[1]])\n","        conv_width_index = tf.tile(tf.expand_dims(conv_width_index, 0), [conv_dim[0], 1])\n","        conv_width_index = K.flatten(tf.transpose(conv_width_index))\n","        conv_index = tf.transpose(tf.stack([conv_height_index, conv_width_index]))\n","        conv_index = tf.reshape(conv_index, (1, conv_dim[0], conv_dim[1], 1, 2), name=\"generateOffsetGrid_reshape_conv_index\")\n","        conv_index = tf.cast(conv_index, dtype, name=\"generateOffsetGrid_cast_conv_index\")\n","        return conv_index\n","\n","    def preprocess_net_output(self, output, anchors, num_classes):\n","        \"\"\"\n","        转换网络最后一层（不包括损失计算层）的输出[batch_size，height_scale, width_scale, num_anchors, num_classes+5]，还原到正常的坐标宽高\n","        :param output: 网络最后一层的输出，不包括损失计算层\n","        :param anchors: 每个grid cell采用5个anchor，[[w,h]]，w，h是与grid cell尺寸的比值\n","        :param num_classes: 类别数量\n","        :return:\n","                box_xy\n","                box_wh\n","                box_conf\n","                box_cls\n","        \"\"\"\n","        num_anchors = len(anchors)\n","        anchors_tensor = tf.reshape(tf.cast(anchors, output.dtype, name=\"preprocess_cast_anchord\"), (1, 1, 1, num_anchors, 2), name=\"preprocess_reshape_anchors\")\n","        conv_dim = output.shape[1:3]\n","\n","        conv_index = self.conv_index\n","        # [batch_size, 13, 13, 125] -> [batch_size, 13, 13, 5, 25]\n","        output = tf.reshape(output, (-1, conv_dim[0], conv_dim[1], num_anchors, num_classes + 5), name=\"preprocess_reshape_output\")\n","        conv_dim = tf.cast(tf.reshape(conv_dim, (1,1,1,1,2), name=\"preprocess_reshape_conv_dim\"), output.dtype, name=\"preprocess_cast_conv_dim\")\n","        # 使用sigmoid缩放到[0, 1]，是因维中心坐标只能在grid cell之内\n","        # (y,x,h,w)\n","        box_xy = tf.sigmoid(output[..., :2])\n","        box_wh = tf.exp(output[..., 2:4])\n","        box_conf = tf.sigmoid(output[..., 4:5])\n","        box_cls = tf.nn.softmax(tf.cast(output[..., 5:], tf.float32), name=\"preprocess_net_output_softmax\")\n","        # 加上grid cell的偏置，除以conv_dim，进行归一化\n","        box_xy = (box_xy + conv_index)/conv_dim\n","        # 论文的公式\n","        box_wh = (box_wh * anchors_tensor)/conv_dim\n","\n","        return box_xy, box_wh, box_conf, box_cls\n","\n","    def yolo_loss(self, args, anchors, num_classes):\n","        \"\"\"\n","        计算损失\n","        :param args:    yolo_out:网络输出，形如(batch_size, 13, 13, 5 * (5+num_classes)), 归一化之后的值\n","                        gt_boxes:真实边界框，形如(batch_size, boxes_per_image, 5)，(x_center, y_center, w, h)是除以原始图片尺寸，归一化之后的值\n","                        response_anchor:用来负责预测的anchor，0/1 负责预测/不负责不预测, 形如（batch_size, 13，13，5，1）\n","                        matching_true_boxes：负责预测的anchor对于gt box的偏移（相对于grid cell）以及目标类别，形如（batch_size, 13，13，5，5）\n","        :param anchors: 一组（5个）先验anchor的尺寸\n","        :param num_classes: 检测目标的类别数\n","        :return:\n","        \"\"\"\n","        yolo_out, gt_boxes, response_anchor, matching_true_boxes = args\n","        num_anchors = len(anchors)\n","\n","        # 各个损失的权重\n","        obj_scale = self.lambda_obj\n","        no_obj_scal = self.lambda_noobj\n","        class_scale = self.lambda_class\n","        coord_scale = self.lambda_coord\n","        \"\"\"\n","        1.如果某一个预测框的Max IOU小于0.6(说明它预测背景)且不匹配ground truth, 则这个预测框计算置信度损失。\n","        2.如果某一个预测框的Max IOU大于0.6，但是不和ground truth匹配，我们忽略其损失。\n","        3.如果某一个预测框匹配ground truth。那么我们就要对这个预测框计算坐标、置信度和类别概率损失。\n","        \"\"\"\n","        \"\"\"\n","        〇 预处理网络的输出\n","        \"\"\"\n","\n","        \"\"\"\n","        pred_xy: [batch_size, 13, 13, 5, 2]\n","        pred_wh: [batch_size, 13, 13, 5, 2]\n","        pred_conf: [batch_size, 13, 13, 5, 1]\n","        pred_cls: [batch_size, 13, 13, 5, 20]\n","        \"\"\"\n","        pred_xy, pred_wh, pred_conf, pred_cls = self.preprocess_net_output(yolo_out, anchors, num_classes)\n","\n","        yolo_out_shape = yolo_out.shape\n","        # shape(batch_size, 13, 13, 5, 25)\n","        output = tf.reshape(yolo_out, (-1, yolo_out_shape[1], yolo_out_shape[2], num_anchors, num_classes+5), name=\"loss_reshape_ouput\")\n","        '''\n","        I 计算IOU\n","        '''\n","        # 增加维度，以便于和gt_boxes计算交叉面积\n","        # shape (batch_size, 13, 13, 5, 1, 2)\n","        pred_xy = tf.expand_dims(pred_xy, 4)\n","        # shape (batch_size, 13, 13, 5, 1, 2)\n","        pred_wh = tf.expand_dims(pred_wh, 4)\n","        # shape (batch_size, 13, 13, 5, 1, 2)\n","        pred_wh_half = pred_wh/2\n","        # 左上右下角坐标\n","        # shape (batch_size, 13, 13, 5, 1, 2)\n","        pred_lu = pred_xy - pred_wh_half\n","        pred_rd = pred_xy + pred_wh_half\n","        # 宽度与高度\n","        # shape(batch_size, 13, 13, 5, 1)\n","        pred_w = pred_wh[..., 0]\n","        pred_h = pred_wh[..., 1]\n","        # 预测框面积\n","        # shape (batch_size, 13, 13, 5, 1)\n","        pred_area = pred_w * pred_h\n","\n","        # shape (batch_size, box_num(42), 5)\n","        gt_boxes_shape = tf.shape(gt_boxes)\n","        # shape (batch_size, 1, 1, 1, box_num(42), 5)\n","        gt_boxes = tf.reshape(gt_boxes, (gt_boxes_shape[0], 1, 1, 1, gt_boxes_shape[1], gt_boxes_shape[2]))\n","        # shape (batch_size, 1, 1, 1, box_num(42), 2)\n","        gt_xy = gt_boxes[..., 0:2]\n","        gt_wh = gt_boxes[..., 2:4]\n","\n","        gt_wh_self = gt_wh/2\n","        # 左上右下角的坐标\n","        # shape (batch_size, 1, 1, 1, box_num(42), 2)\n","        gt_lu = gt_xy - gt_wh_self\n","        gt_rd = gt_xy + gt_wh_self\n","        # 宽度与高度\n","        # shape (batch_szie, 1, 1, 1, box_num(42))\n","        gt_h = gt_wh[..., 0]\n","        gt_w = gt_wh[..., 1]\n","        # shape (batch_szie, 1, 1, 1, box_num(42))\n","        gt_area = gt_w * gt_h\n","        # 计算交叉区域面积\n","        # 左上、右下角坐标\n","        # shape (batch_szie, 13, 13, 5, box_num(42), 2)\n","        inter_min = tf.maximum(pred_lu, gt_lu)\n","        inter_max = tf.minimum(pred_rd, gt_rd)\n","        inter_wh = tf.maximum(inter_max - inter_min, 0.)\n","        # 交叉区域的宽度与高度\n","        # shape (batch_size, 13, 13, 5, box_num(42))\n","        inter_h = inter_wh[..., 0]\n","        inter_w = inter_wh[..., 1]\n","        inter_area = inter_w * inter_h\n","        union_area = pred_area + gt_area - inter_area\n","        # shape (batch_size, 13, 13, 5, box_num(42))\n","        iou = inter_area/union_area\n","        \"\"\"\n","        II 计算置信度损失\n","        负责预测目标的anchor置信度 + 不负责预测目标的anchor置信度\n","        \"\"\"\n","        # shape (batch_size, 13, 13, 5, 1)\n","        obj_detection = tf.cast(iou > 0.6, iou.dtype, name=\"loss_cast_iou\")\n","        no_obj_detection = 1 - obj_detection\n","\n","        # shape (batch_size, 13, 13, 5, 1)\n","        no_obj_weight = no_obj_scal * no_obj_detection * (1 - response_anchor)\n","        # 对于不负责预测目标的anchor，其置信度要接近于0\n","        no_obj_conf_loss = no_obj_weight * tf.square(pred_conf)\n","        obj_conf_loss = obj_scale * response_anchor * tf.square(pred_conf - iou)\n","        # shape (batch_size, 13, 13, 5, 1)\n","        conf_loss = no_obj_conf_loss + obj_conf_loss\n","        \"\"\"\n","        III计算坐标损失\n","        负责预测的anchor的坐标损失\n","        不负责预测的anchors的坐标损失？？？？\n","        \"\"\"\n","        # 得到x,y,w,h坐标用于计算坐标损失\n","        # shape (batch_size, 13, 13, 5, 4)\n","        pred_boxes = tf.concat([tf.sigmoid(output[..., 0:2]), output[..., 2:4]], axis=-1)\n","        # shape (batch_size, 13, 13, 5, 4)\n","        matching_boxes = matching_true_boxes[..., 0:4]\n","        coord_loss = coord_scale * response_anchor * tf.square(matching_boxes - pred_boxes)\n","        \"\"\"\n","        IV计算预测类别损失\n","        \"\"\"\n","        # shape (batch_size, 13, 13, 5)\n","        true_cls = matching_true_boxes[..., 4]\n","        # shape (batch_size, 13, 13, 5, 20)\n","        true_class = tf.one_hot(tf.cast(true_cls, tf.int32), num_classes)\n","        # shape (batch_size, 13, 13, 5, 20)\n","        class_loss = class_scale * response_anchor * tf.square(tf.cast(pred_cls, dtype=tf.float32) - tf.cast(true_class, tf.float32))\n","        \"\"\"\n","        V计算总体损失\n","        \"\"\"\n","        conf_loss_sum = tf.reduce_sum(conf_loss, axis=[1,2,3,4])\n","        coord_loss_sum = tf.reduce_sum(coord_loss, axis=[1,2,3,4])\n","        class_loss_sum = tf.reduce_sum(class_loss, axis=[1,2,3,4])\n","\n","        total_loss = 0.5 * (conf_loss_sum + coord_loss_sum + class_loss_sum)\n","        total_loss = tf.reduce_sum(total_loss, keepdims=True)\n","        tf.print(\"total_loss\", total_loss, output_stream=sys.stderr)\n","        return total_loss"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Exx1pbdFlzAq","executionInfo":{"status":"ok","timestamp":1608215130094,"user_tz":-480,"elapsed":1641,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}},"outputId":"8df8ff2e-0238-49d8-eae1-513762bc5775"},"source":["yolov2 = YOLOv2(alpha=0.1, model_path=\"./drive/My Drive/PapperReproduce/YOLOv2/yolo_v2_model_weights.h5\")\n","# yolov2 = YOLOv2()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["model training......\n","load model weights......\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uyw1OOlL-DXT"},"source":["# modelcheck = keras.callbacks.ModelCheckpoint(\"weights_{epoch:03d}-{val_loss:.4f}.h5\",\n","#                                     monitor='val_loss',\n","#                                     verbose=0,\n","#                                     save_best_only=True,\n","#                                     save_weights_only=False,\n","#                                     mode='auto',\n","#                                     period=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MgLZupfA-Ei","executionInfo":{"status":"ok","timestamp":1608215133921,"user_tz":-480,"elapsed":804,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}}},"source":["yolov2.compile_model()"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"J129SqXxLboH","executionInfo":{"status":"ok","timestamp":1608201448443,"user_tz":-480,"elapsed":1191,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}}},"source":["def get_train_data_by_batch():\n","    while 1:\n","        for i in range(0, len(pascal.gt_labels), cfg.BATCH_SIZE):\n","            images, matching_true_boxes, gt_boxes, response_anchors = pascal.get()\n","            y = np.zeros(len(images))\n","            yield ([images, gt_boxes, response_anchors, matching_true_boxes], y)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"i6zxHdoDEEZS","executionInfo":{"status":"error","timestamp":1608215763044,"user_tz":-480,"elapsed":627269,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}},"outputId":"e18c4296-c98e-4fea-c45a-a30c80d316eb"},"source":["yolov2.train_generator(get_train_data_by_batch, len(pascal.gt_labels))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Epoch 1/90\n","total_loss [99.3246613]\n"," 1/78 [..............................] - ETA: 0s - loss: 99.3247 - mse: 9865.3887total_loss [124.356667]\n"," 2/78 [..............................] - ETA: 1:12 - loss: 111.8407 - mse: 12664.9844total_loss [86.2701416]\n"," 3/78 [>.............................] - ETA: 1:34 - loss: 103.3172 - mse: 10924.1689total_loss [103.798874]\n"," 4/78 [>.............................] - ETA: 1:45 - loss: 103.4376 - mse: 10886.6787total_loss [108.426041]\n"," 5/78 [>.............................] - ETA: 1:51 - loss: 104.4353 - mse: 11060.5840total_loss [103.017883]\n"," 6/78 [=>............................] - ETA: 1:55 - loss: 104.1990 - mse: 10985.9346total_loss [88.2004471]\n"," 7/78 [=>............................] - ETA: 1:57 - loss: 101.9135 - mse: 10527.8467total_loss [103.684097]\n"," 8/78 [==>...........................] - ETA: 1:58 - loss: 102.1348 - mse: 10555.6650total_loss [116.755676]\n"," 9/78 [==>...........................] - ETA: 1:59 - loss: 103.7594 - mse: 10897.4678total_loss [88.7756653]\n","10/78 [==>...........................] - ETA: 1:59 - loss: 102.2610 - mse: 10595.8330total_loss [114.197372]\n","11/78 [===>..........................] - ETA: 1:58 - loss: 103.3461 - mse: 10818.1240total_loss [102.669006]\n","12/78 [===>..........................] - ETA: 1:57 - loss: 103.2897 - mse: 10795.0244total_loss [99.3330688]\n","13/78 [====>.........................] - ETA: 1:56 - loss: 102.9854 - mse: 10723.6416total_loss [106.156662]\n","14/78 [====>.........................] - ETA: 1:54 - loss: 103.2119 - mse: 10762.6123total_loss [98.9706879]\n","15/78 [====>.........................] - ETA: 1:53 - loss: 102.9291 - mse: 10698.1191total_loss [100.809235]\n","16/78 [=====>........................] - ETA: 1:51 - loss: 102.7966 - mse: 10664.6426total_loss [96.6051331]\n","17/78 [=====>........................] - ETA: 1:50 - loss: 102.4324 - mse: 10586.2842total_loss [100.340347]\n","18/78 [=====>........................] - ETA: 1:48 - loss: 102.3162 - mse: 10557.5010total_loss [104.287453]\n","19/78 [======>.......................] - ETA: 1:46 - loss: 102.4199 - mse: 10574.2578total_loss [87.8352203]\n","20/78 [======>.......................] - ETA: 1:45 - loss: 101.6907 - mse: 10431.2959total_loss [103.844666]\n","21/78 [=======>......................] - ETA: 1:43 - loss: 101.7933 - mse: 10448.0781total_loss [96.1065]\n","22/78 [=======>......................] - ETA: 1:41 - loss: 101.5348 - mse: 10393.0039total_loss [87.1652374]\n","23/78 [=======>......................] - ETA: 1:39 - loss: 100.9100 - mse: 10271.4727total_loss [115.960236]\n","24/78 [========>.....................] - ETA: 1:37 - loss: 101.5371 - mse: 10403.7773total_loss [89.3716049]\n","25/78 [========>.....................] - ETA: 1:36 - loss: 101.0505 - mse: 10307.1172total_loss [96.6974335]\n","26/78 [=========>....................] - ETA: 1:34 - loss: 100.8831 - mse: 10270.3213total_loss [106.089874]\n","27/78 [=========>....................] - ETA: 1:32 - loss: 101.0759 - mse: 10306.7930total_loss [100.826637]\n","28/78 [=========>....................] - ETA: 1:30 - loss: 101.0670 - mse: 10301.7646total_loss [110.682556]\n","29/78 [==========>...................] - ETA: 1:28 - loss: 101.3986 - mse: 10368.9668total_loss [103.575356]\n","30/78 [==========>...................] - ETA: 1:26 - loss: 101.4712 - mse: 10380.9287total_loss [86.1250305]\n","31/78 [==========>...................] - ETA: 1:25 - loss: 100.9761 - mse: 10285.3359total_loss [113.104797]\n","32/78 [===========>..................] - ETA: 1:23 - loss: 101.3551 - mse: 10363.6904total_loss [119.684326]\n","33/78 [===========>..................] - ETA: 1:21 - loss: 101.9106 - mse: 10483.7100total_loss [99.5093384]\n","34/78 [============>.................] - ETA: 1:19 - loss: 101.8399 - mse: 10466.6035total_loss [132.451324]\n","35/78 [============>.................] - ETA: 1:17 - loss: 102.7146 - mse: 10668.7969total_loss [83.5354385]\n","36/78 [============>.................] - ETA: 1:15 - loss: 102.1818 - mse: 10566.2783total_loss [91.1737061]\n","37/78 [=============>................] - ETA: 1:14 - loss: 101.8843 - mse: 10505.3701total_loss [92.4573593]\n","38/78 [=============>................] - ETA: 1:12 - loss: 101.6362 - mse: 10453.8701total_loss [115.621902]\n","39/78 [==============>...............] - ETA: 1:10 - loss: 101.9948 - mse: 10528.6025total_loss [90.7229309]\n","40/78 [==============>...............] - ETA: 1:08 - loss: 101.7130 - mse: 10471.1543total_loss [86.9676056]\n","41/78 [==============>...............] - ETA: 1:06 - loss: 101.3534 - mse: 10400.2324total_loss [111.607773]\n","42/78 [===============>..............] - ETA: 1:05 - loss: 101.5975 - mse: 10449.1855total_loss [92.7556839]\n","43/78 [===============>..............] - ETA: 1:03 - loss: 101.3919 - mse: 10406.2656total_loss [119.591354]\n","44/78 [===============>..............] - ETA: 1:01 - loss: 101.8055 - mse: 10494.8076total_loss [92.3052368]\n","45/78 [================>.............] - ETA: 59s - loss: 101.5944 - mse: 10450.9287 total_loss [96.448761]\n","46/78 [================>.............] - ETA: 57s - loss: 101.4825 - mse: 10425.9600total_loss [113.231377]\n","47/78 [=================>............] - ETA: 56s - loss: 101.7325 - mse: 10476.9258total_loss [105.376976]\n","48/78 [=================>............] - ETA: 54s - loss: 101.8084 - mse: 10489.9961total_loss [109.36705]\n","49/78 [=================>............] - ETA: 52s - loss: 101.9627 - mse: 10520.0195total_loss [113.173683]\n","50/78 [==================>...........] - ETA: 50s - loss: 102.1869 - mse: 10565.7852total_loss [116.824707]\n","51/78 [==================>...........] - ETA: 49s - loss: 102.4739 - mse: 10626.2207total_loss [127.477905]\n","52/78 [===================>..........] - ETA: 47s - loss: 102.9548 - mse: 10734.3818total_loss [118.689293]\n","53/78 [===================>..........] - ETA: 45s - loss: 103.2517 - mse: 10797.6416total_loss [110.322784]\n","54/78 [===================>..........] - ETA: 43s - loss: 103.3826 - mse: 10823.0762total_loss [120.522415]\n","55/78 [====================>.........] - ETA: 41s - loss: 103.6942 - mse: 10890.3955total_loss [90.1154633]\n","56/78 [====================>.........] - ETA: 40s - loss: 103.4518 - mse: 10840.9385total_loss [136.629593]\n","57/78 [====================>.........] - ETA: 38s - loss: 104.0338 - mse: 10978.2490total_loss [91.0486]\n","58/78 [=====================>........] - ETA: 36s - loss: 103.8100 - mse: 10931.8975total_loss [110.351654]\n","59/78 [=====================>........] - ETA: 34s - loss: 103.9208 - mse: 10953.0098total_loss [115.792068]\n","60/78 [======================>.......] - ETA: 32s - loss: 104.1187 - mse: 10993.9229total_loss [131.882172]\n","61/78 [======================>.......] - ETA: 30s - loss: 104.5738 - mse: 11098.8242total_loss [107.983185]\n","62/78 [======================>.......] - ETA: 29s - loss: 104.6288 - mse: 11107.8809total_loss [89.5977249]\n","63/78 [=======================>......] - ETA: 27s - loss: 104.3902 - mse: 11058.9902total_loss [100.944733]\n","64/78 [=======================>......] - ETA: 25s - loss: 104.3364 - mse: 11045.4092total_loss [98.4131317]\n","65/78 [========================>.....] - ETA: 23s - loss: 104.2453 - mse: 11024.4814total_loss [80.0039368]\n","66/78 [========================>.....] - ETA: 21s - loss: 103.8780 - mse: 10954.4229total_loss [88.6494446]\n","67/78 [========================>.....] - ETA: 20s - loss: 103.6507 - mse: 10908.2188total_loss [95.0529175]\n","68/78 [=========================>....] - ETA: 18s - loss: 103.5242 - mse: 10880.6729total_loss [115.30632]\n","69/78 [=========================>....] - ETA: 16s - loss: 103.6950 - mse: 10915.6709total_loss [95.1390839]\n","70/78 [=========================>....] - ETA: 14s - loss: 103.5728 - mse: 10889.0391total_loss [106.931839]\n","71/78 [==========================>...] - ETA: 12s - loss: 103.6201 - mse: 10896.7207total_loss [78.1685181]\n","72/78 [==========================>...] - ETA: 10s - loss: 103.2666 - mse: 10830.2432total_loss [107.188553]\n","73/78 [===========================>..] - ETA: 9s - loss: 103.3203 - mse: 10839.2725 total_loss [95.7647247]\n","74/78 [===========================>..] - ETA: 7s - loss: 103.2182 - mse: 10816.7266total_loss [89.9107361]\n","75/78 [===========================>..] - ETA: 5s - loss: 103.0408 - mse: 10780.2891total_loss [85.7411652]\n","76/78 [============================>.] - ETA: 3s - loss: 102.8131 - mse: 10735.1748total_loss [125.909897]\n","77/78 [============================>.] - ETA: 1s - loss: 103.1131 - mse: 10801.6436total_loss [92.3492584]\n","78/78 [==============================] - 142s 2s/step - loss: 102.9751 - mse: 10772.4990\n","Epoch 2/90\n","total_loss [120.453461]\n"," 1/78 [..............................] - ETA: 0s - loss: 120.4535 - mse: 14509.0361total_loss [95.9667358]\n"," 2/78 [..............................] - ETA: 1:07 - loss: 108.2101 - mse: 11859.3252total_loss [125.255928]\n"," 3/78 [>.............................] - ETA: 1:30 - loss: 113.8920 - mse: 13135.8994total_loss [109.720123]\n"," 4/78 [>.............................] - ETA: 1:40 - loss: 112.8491 - mse: 12861.5508total_loss [96.2084198]\n"," 5/78 [>.............................] - ETA: 1:46 - loss: 109.5209 - mse: 12140.4521total_loss [116.427185]\n"," 6/78 [=>............................] - ETA: 1:48 - loss: 110.6720 - mse: 12376.2578total_loss [109.196373]\n"," 7/78 [=>............................] - ETA: 1:50 - loss: 110.4612 - mse: 12311.6270total_loss [106.519745]\n"," 8/78 [==>...........................] - ETA: 1:51 - loss: 109.9685 - mse: 12190.9805total_loss [89.2166214]\n"," 9/78 [==>...........................] - ETA: 1:52 - loss: 107.6627 - mse: 11720.8281total_loss [108.09108]\n","10/78 [==>...........................] - ETA: 1:52 - loss: 107.7056 - mse: 11717.1133total_loss [90.1585]\n","11/78 [===>..........................] - ETA: 1:51 - loss: 106.1104 - mse: 11390.8809total_loss [121.801369]\n","12/78 [===>..........................] - ETA: 1:50 - loss: 107.4180 - mse: 11677.9385total_loss [116.870461]\n","13/78 [====>.........................] - ETA: 1:50 - loss: 108.1451 - mse: 11830.3057total_loss [102.781853]\n","14/78 [====>.........................] - ETA: 1:49 - loss: 107.7620 - mse: 11739.8623total_loss [104.539139]\n","15/78 [====>.........................] - ETA: 1:48 - loss: 107.5471 - mse: 11685.7676total_loss [93.407608]\n","16/78 [=====>........................] - ETA: 1:46 - loss: 106.6634 - mse: 11500.7188total_loss [87.3726425]\n","17/78 [=====>........................] - ETA: 1:45 - loss: 105.5287 - mse: 11273.2637total_loss [91.9128265]\n","18/78 [=====>........................] - ETA: 1:44 - loss: 104.7722 - mse: 11116.3027total_loss [105.901047]\n","19/78 [======>.......................] - ETA: 1:42 - loss: 104.8316 - mse: 11121.4990total_loss [89.2655487]\n","20/78 [======>.......................] - ETA: 1:40 - loss: 104.0533 - mse: 10963.8418total_loss [92.0510559]\n","21/78 [=======>......................] - ETA: 1:39 - loss: 103.4818 - mse: 10845.2480total_loss [122.711014]\n","22/78 [=======>......................] - ETA: 1:38 - loss: 104.3558 - mse: 11036.7373total_loss [102.387596]\n","23/78 [=======>......................] - ETA: 1:36 - loss: 104.2703 - mse: 11012.6709total_loss [117.006973]\n","24/78 [========>.....................] - ETA: 1:34 - loss: 104.8010 - mse: 11124.2529total_loss [101.540817]\n","25/78 [========>.....................] - ETA: 1:33 - loss: 104.6706 - mse: 11091.7041total_loss [109.687622]\n","26/78 [=========>....................] - ETA: 1:31 - loss: 104.8635 - mse: 11127.8447total_loss [99.6981659]\n","27/78 [=========>....................] - ETA: 1:30 - loss: 104.6722 - mse: 11083.8398total_loss [106.639862]\n","28/78 [=========>....................] - ETA: 1:28 - loss: 104.7425 - mse: 11094.1338total_loss [95.8946686]\n","29/78 [==========>...................] - ETA: 1:26 - loss: 104.4374 - mse: 11028.6738total_loss [114.69809]\n","30/78 [==========>...................] - ETA: 1:25 - loss: 104.7794 - mse: 11099.5732total_loss [94.959]\n","31/78 [==========>...................] - ETA: 1:23 - loss: 104.4626 - mse: 11032.4004total_loss [112.794296]\n","32/78 [===========>..................] - ETA: 1:21 - loss: 104.7230 - mse: 11085.2178total_loss [115.751526]\n","33/78 [===========>..................] - ETA: 1:19 - loss: 105.0572 - mse: 11155.3145total_loss [91.2414703]\n","34/78 [============>.................] - ETA: 1:18 - loss: 104.6508 - mse: 11072.0703total_loss [114.519714]\n","35/78 [============>.................] - ETA: 1:16 - loss: 104.9328 - mse: 11130.4326total_loss [120.612122]\n","36/78 [============>.................] - ETA: 1:14 - loss: 105.3683 - mse: 11225.3447total_loss [87.5422363]\n","37/78 [=============>................] - ETA: 1:13 - loss: 104.8866 - mse: 11129.0830total_loss [114.194641]\n","38/78 [=============>................] - ETA: 1:11 - loss: 105.1315 - mse: 11179.3809total_loss [106.126785]\n","39/78 [==============>...............] - ETA: 1:09 - loss: 105.1570 - mse: 11181.5225total_loss [100.499275]\n","40/78 [==============>...............] - ETA: 1:07 - loss: 105.0406 - mse: 11154.4863total_loss [94.8062592]\n","41/78 [==============>...............] - ETA: 1:06 - loss: 104.7910 - mse: 11101.6514total_loss [89.7481384]\n","42/78 [===============>..............] - ETA: 1:04 - loss: 104.4328 - mse: 11029.1045total_loss [107.527939]\n","43/78 [===============>..............] - ETA: 1:02 - loss: 104.5048 - mse: 11041.5039total_loss [114.123901]\n","44/78 [===============>..............] - ETA: 1:00 - loss: 104.7234 - mse: 11086.5664total_loss [105.972046]\n","45/78 [================>.............] - ETA: 59s - loss: 104.7512 - mse: 11089.7549 total_loss [95.4545135]\n","46/78 [================>.............] - ETA: 57s - loss: 104.5490 - mse: 11046.7510total_loss [112.979897]\n","47/78 [=================>............] - ETA: 55s - loss: 104.7284 - mse: 11083.2979total_loss [90.5132]\n","48/78 [=================>............] - ETA: 53s - loss: 104.4323 - mse: 11023.0752total_loss [94.7158585]\n","49/78 [=================>............] - ETA: 52s - loss: 104.2340 - mse: 10981.1992total_loss [93.4869385]\n","50/78 [==================>...........] - ETA: 50s - loss: 104.0190 - mse: 10936.3711total_loss [102.17041]\n","51/78 [==================>...........] - ETA: 48s - loss: 103.9828 - mse: 10926.6152total_loss [98.4893494]\n","52/78 [===================>..........] - ETA: 46s - loss: 103.8772 - mse: 10903.0293total_loss [101.946503]\n","53/78 [===================>..........] - ETA: 44s - loss: 103.8407 - mse: 10893.4072total_loss [129.680664]\n","54/78 [===================>..........] - ETA: 43s - loss: 104.3192 - mse: 11003.1045total_loss [107.918152]\n","55/78 [====================>.........] - ETA: 41s - loss: 104.3847 - mse: 11014.7988total_loss [85.0445251]\n","56/78 [====================>.........] - ETA: 39s - loss: 104.0393 - mse: 10947.2588total_loss [88.105423]\n","57/78 [====================>.........] - ETA: 37s - loss: 103.7598 - mse: 10891.3867total_loss [101.822693]\n","58/78 [=====================>........] - ETA: 36s - loss: 103.7264 - mse: 10882.3613total_loss [117.139633]\n","59/78 [=====================>........] - ETA: 34s - loss: 103.9537 - mse: 10930.4854total_loss [113.737198]\n","60/78 [======================>.......] - ETA: 32s - loss: 104.1168 - mse: 10963.9121total_loss [116.631912]\n","61/78 [======================>.......] - ETA: 30s - loss: 104.3219 - mse: 11007.1758total_loss [106.797729]\n","62/78 [======================>.......] - ETA: 28s - loss: 104.3619 - mse: 11013.6045total_loss [110.545807]\n","63/78 [=======================>......] - ETA: 27s - loss: 104.4600 - mse: 11032.7598total_loss [90.1556091]\n","64/78 [=======================>......] - ETA: 25s - loss: 104.2365 - mse: 10987.3740total_loss [95.8630676]\n","65/78 [========================>.....] - ETA: 23s - loss: 104.1077 - mse: 10959.7188total_loss [110.247269]\n","66/78 [========================>.....] - ETA: 21s - loss: 104.2007 - mse: 10977.8203total_loss [94.6033401]\n","67/78 [========================>.....] - ETA: 19s - loss: 104.0575 - mse: 10947.5518total_loss [93.0515671]\n","68/78 [=========================>....] - ETA: 18s - loss: 103.8956 - mse: 10913.8896total_loss [118.907578]\n","69/78 [=========================>....] - ETA: 16s - loss: 104.1132 - mse: 10960.6309total_loss [119.059326]\n","70/78 [=========================>....] - ETA: 14s - loss: 104.3267 - mse: 11006.5518total_loss [108.295113]\n","71/78 [==========================>...] - ETA: 12s - loss: 104.3826 - mse: 11016.7100total_loss [105.546021]\n","72/78 [==========================>...] - ETA: 10s - loss: 104.3988 - mse: 11018.4219total_loss [114.776123]\n","73/78 [===========================>..] - ETA: 9s - loss: 104.5409 - mse: 11047.9443 total_loss [101.732193]\n","74/78 [===========================>..] - ETA: 7s - loss: 104.5030 - mse: 11038.5049total_loss [109.323288]\n","75/78 [===========================>..] - ETA: 5s - loss: 104.5672 - mse: 11050.6787total_loss [112.804703]\n","76/78 [============================>.] - ETA: 3s - loss: 104.6756 - mse: 11072.7080total_loss [100.626816]\n","77/78 [============================>.] - ETA: 1s - loss: 104.6230 - mse: 11060.4102total_loss [95.082016]\n","78/78 [==============================] - 141s 2s/step - loss: 104.5007 - mse: 11034.5146\n","Epoch 3/90\n","total_loss [104.210564]\n"," 1/78 [..............................] - ETA: 0s - loss: 104.2106 - mse: 10859.8418total_loss [103.562668]\n"," 2/78 [..............................] - ETA: 1:08 - loss: 103.8866 - mse: 10792.5342total_loss [109.837074]\n"," 3/78 [>.............................] - ETA: 1:31 - loss: 105.8701 - mse: 11216.4170total_loss [128.861023]\n"," 4/78 [>.............................] - ETA: 1:41 - loss: 111.6178 - mse: 12563.6035total_loss [126.718155]\n"," 5/78 [>.............................] - ETA: 1:46 - loss: 114.6379 - mse: 13262.3809total_loss [105.283264]\n"," 6/78 [=>............................] - ETA: 1:49 - loss: 113.0788 - mse: 12899.4111total_loss [105.248283]\n"," 7/78 [=>............................] - ETA: 1:50 - loss: 111.9601 - mse: 12639.0957total_loss [88.9986343]\n"," 8/78 [==>...........................] - ETA: 1:51 - loss: 109.0900 - mse: 12049.3037total_loss [103.274498]\n"," 9/78 [==>...........................] - ETA: 1:51 - loss: 108.4438 - mse: 11895.5615total_loss [97.1409149]\n","10/78 [==>...........................] - ETA: 1:51 - loss: 107.3135 - mse: 11649.6416total_loss [71.2177887]\n","11/78 [===>..........................] - ETA: 1:50 - loss: 104.0321 - mse: 11051.6719total_loss [99.441925]\n","12/78 [===>..........................] - ETA: 1:50 - loss: 103.6496 - mse: 10954.7578total_loss [100.160606]\n","13/78 [====>.........................] - ETA: 1:49 - loss: 103.3812 - mse: 10883.7871total_loss [120.356026]\n","14/78 [====>.........................] - ETA: 1:48 - loss: 104.5937 - mse: 11141.0576total_loss [92.4032]\n","15/78 [====>.........................] - ETA: 1:47 - loss: 103.7810 - mse: 10967.5439total_loss [105.331924]\n","16/78 [=====>........................] - ETA: 1:46 - loss: 103.8779 - mse: 10975.4980total_loss [130.68573]\n","17/78 [=====>........................] - ETA: 1:45 - loss: 105.4548 - mse: 11334.5137total_loss [105.481636]\n","18/78 [=====>........................] - ETA: 1:43 - loss: 105.4563 - mse: 11322.9502total_loss [96.666893]\n","19/78 [======>.......................] - ETA: 1:42 - loss: 104.9937 - mse: 11218.8203total_loss [102.470039]\n","20/78 [======>.......................] - ETA: 1:40 - loss: 104.8675 - mse: 11182.8848total_loss [114.411606]\n","21/78 [=======>......................] - ETA: 1:39 - loss: 105.3220 - mse: 11273.7012total_loss [86.513092]\n","22/78 [=======>......................] - ETA: 1:37 - loss: 104.4671 - mse: 11101.4648total_loss [101.629066]\n","23/78 [=======>......................] - ETA: 1:36 - loss: 104.3437 - mse: 11067.8564total_loss [117.178925]\n","24/78 [========>.....................] - ETA: 1:34 - loss: 104.8785 - mse: 11178.8164total_loss [108.056488]\n","25/78 [========>.....................] - ETA: 1:33 - loss: 105.0056 - mse: 11198.7129total_loss [97.631752]\n","26/78 [=========>....................] - ETA: 1:31 - loss: 104.7220 - mse: 11134.6074total_loss [96.0742]\n","27/78 [=========>....................] - ETA: 1:29 - loss: 104.4017 - mse: 11064.0752total_loss [111.479218]\n","28/78 [=========>....................] - ETA: 1:28 - loss: 104.6545 - mse: 11112.7734total_loss [104.453568]\n","29/78 [==========>...................] - ETA: 1:26 - loss: 104.6476 - mse: 11105.8008total_loss [110.482315]\n","30/78 [==========>...................] - ETA: 1:24 - loss: 104.8420 - mse: 11142.4854total_loss [135.962097]\n","31/78 [==========>...................] - ETA: 1:23 - loss: 105.8459 - mse: 11379.3633total_loss [88.8433]\n","32/78 [===========>..................] - ETA: 1:21 - loss: 105.3146 - mse: 11270.4180total_loss [78.3827667]\n","33/78 [===========>..................] - ETA: 1:19 - loss: 104.4985 - mse: 11115.0674total_loss [110.536026]\n","34/78 [============>.................] - ETA: 1:18 - loss: 104.6761 - mse: 11147.5127total_loss [106.57486]\n","35/78 [============>.................] - ETA: 1:16 - loss: 104.7303 - mse: 11153.5322total_loss [93.4356842]\n","36/78 [============>.................] - ETA: 1:14 - loss: 104.4166 - mse: 11086.2178total_loss [110.845352]\n","37/78 [=============>................] - ETA: 1:13 - loss: 104.5903 - mse: 11118.6631total_loss [93.7155]\n","38/78 [=============>................] - ETA: 1:11 - loss: 104.3041 - mse: 11057.1875total_loss [93.9372482]\n","39/78 [==============>...............] - ETA: 1:09 - loss: 104.0383 - mse: 10999.9316total_loss [112.279694]\n","40/78 [==============>...............] - ETA: 1:07 - loss: 104.2444 - mse: 11040.1016total_loss [131.494858]\n","41/78 [==============>...............] - ETA: 1:06 - loss: 104.9090 - mse: 11192.5605total_loss [87.0629883]\n","42/78 [===============>..............] - ETA: 1:04 - loss: 104.4841 - mse: 11106.5459total_loss [89.952652]\n","43/78 [===============>..............] - ETA: 1:02 - loss: 104.1462 - mse: 11036.4277total_loss [95.9305267]\n","44/78 [===============>..............] - ETA: 1:00 - loss: 103.9594 - mse: 10994.7510total_loss [127.631912]\n","45/78 [================>.............] - ETA: 59s - loss: 104.4855 - mse: 11112.4219 total_loss [98.5667725]\n","46/78 [================>.............] - ETA: 57s - loss: 104.3568 - mse: 11082.0518total_loss [104.406113]\n","47/78 [=================>............] - ETA: 55s - loss: 104.3579 - mse: 11078.1914total_loss [116.713722]\n","48/78 [=================>............] - ETA: 53s - loss: 104.6153 - mse: 11131.1885total_loss [112.80069]\n","49/78 [=================>............] - ETA: 52s - loss: 104.7823 - mse: 11163.6953total_loss [92.9852]\n","50/78 [==================>...........] - ETA: 50s - loss: 104.5464 - mse: 11113.3467total_loss [102.64035]\n","51/78 [==================>...........] - ETA: 48s - loss: 104.5090 - mse: 11102.0078total_loss [112.228439]\n","52/78 [===================>..........] - ETA: 46s - loss: 104.6575 - mse: 11130.7236total_loss [115.556732]\n","53/78 [===================>..........] - ETA: 44s - loss: 104.8631 - mse: 11172.6602total_loss [96.0988846]\n","54/78 [===================>..........] - ETA: 43s - loss: 104.7008 - mse: 11136.7773total_loss [125.048149]\n","55/78 [====================>.........] - ETA: 41s - loss: 105.0708 - mse: 11218.6016total_loss [105.760803]\n","56/78 [====================>.........] - ETA: 39s - loss: 105.0831 - mse: 11218.0078total_loss [93.2823486]\n","57/78 [====================>.........] - ETA: 37s - loss: 104.8761 - mse: 11173.8604total_loss [95.9813766]\n","58/78 [=====================>........] - ETA: 36s - loss: 104.7227 - mse: 11140.0430total_loss [93.6207]\n","59/78 [=====================>........] - ETA: 34s - loss: 104.5345 - mse: 11099.7852total_loss [103.561249]\n","60/78 [======================>.......] - ETA: 32s - loss: 104.5183 - mse: 11093.5371total_loss [92.9377136]\n","61/78 [======================>.......] - ETA: 30s - loss: 104.3285 - mse: 11053.2734total_loss [107.592438]\n","62/78 [======================>.......] - ETA: 28s - loss: 104.3811 - mse: 11061.7070total_loss [118.387421]\n","63/78 [=======================>......] - ETA: 27s - loss: 104.6034 - mse: 11108.5928total_loss [101.527695]\n","64/78 [=======================>......] - ETA: 25s - loss: 104.5554 - mse: 11096.0820total_loss [101.391205]\n","65/78 [========================>.....] - ETA: 23s - loss: 104.5067 - mse: 11083.5303total_loss [90.1193085]\n","66/78 [========================>.....] - ETA: 21s - loss: 104.2887 - mse: 11038.6504total_loss [120.617569]\n","67/78 [========================>.....] - ETA: 19s - loss: 104.5324 - mse: 11091.0381total_loss [94.8122101]\n","68/78 [=========================>....] - ETA: 18s - loss: 104.3895 - mse: 11060.1318total_loss [104.986946]\n","69/78 [=========================>....] - ETA: 16s - loss: 104.3981 - mse: 11059.5820total_loss [105.547615]\n","70/78 [=========================>....] - ETA: 14s - loss: 104.4146 - mse: 11060.7354total_loss [100.933205]\n","71/78 [==========================>...] - ETA: 12s - loss: 104.3655 - mse: 11048.4365total_loss [96.0453339]\n","72/78 [==========================>...] - ETA: 10s - loss: 104.2500 - mse: 11023.1064total_loss [114.525955]\n","73/78 [===========================>..] - ETA: 9s - loss: 104.3907 - mse: 11051.7793 total_loss [96.5175781]\n","74/78 [===========================>..] - ETA: 7s - loss: 104.2843 - mse: 11028.3174total_loss [89.7014236]\n","75/78 [===========================>..] - ETA: 5s - loss: 104.0899 - mse: 10988.5586total_loss [117.858559]\n","76/78 [============================>.] - ETA: 3s - loss: 104.2711 - mse: 11026.7432total_loss [95.9262238]\n","77/78 [============================>.] - ETA: 1s - loss: 104.1627 - mse: 11003.0430total_loss [97.3838348]\n","78/78 [==============================] - 141s 2s/step - loss: 104.0758 - mse: 10983.5635\n","Epoch 4/90\n","total_loss [99.5416718]\n"," 1/78 [..............................] - ETA: 0s - loss: 99.5417 - mse: 9908.5439total_loss [99.7127075]\n"," 2/78 [..............................] - ETA: 1:10 - loss: 99.6272 - mse: 9925.5840total_loss [118.631165]\n"," 3/78 [>.............................] - ETA: 1:31 - loss: 105.9619 - mse: 11308.1748total_loss [109.33226]\n"," 4/78 [>.............................] - ETA: 1:41 - loss: 106.8045 - mse: 11469.5166total_loss [133.306717]\n"," 5/78 [>.............................] - ETA: 1:46 - loss: 112.1049 - mse: 12729.7500total_loss [107.82811]\n"," 6/78 [=>............................] - ETA: 1:49 - loss: 111.3921 - mse: 12545.9414total_loss [92.4327698]\n"," 7/78 [=>............................] - ETA: 1:51 - loss: 108.6836 - mse: 11974.2100total_loss [78.9284363]\n"," 8/78 [==>...........................] - ETA: 1:51 - loss: 104.9642 - mse: 11256.1455total_loss [100.002937]\n"," 9/78 [==>...........................] - ETA: 1:51 - loss: 104.4130 - mse: 11116.6387total_loss [97.0489349]\n","10/78 [==>...........................] - ETA: 1:51 - loss: 103.6766 - mse: 10946.8252total_loss [108.433044]\n","11/78 [===>..........................] - ETA: 1:51 - loss: 104.1090 - mse: 11020.5430total_loss [96.1784363]\n","12/78 [===>..........................] - ETA: 1:50 - loss: 103.4481 - mse: 10873.0225total_loss [115.363243]\n","13/78 [====>.........................] - ETA: 1:49 - loss: 104.3647 - mse: 11060.3799total_loss [94.6363373]\n","14/78 [====>.........................] - ETA: 1:48 - loss: 103.6698 - mse: 10910.0693total_loss [109.342773]\n","15/78 [====>.........................] - ETA: 1:47 - loss: 104.0480 - mse: 10979.7871total_loss [112.383148]\n","16/78 [=====>........................] - ETA: 1:45 - loss: 104.5689 - mse: 11082.9238total_loss [106.884842]\n","17/78 [=====>........................] - ETA: 1:44 - loss: 104.7052 - mse: 11103.0088total_loss [115.97435]\n","18/78 [=====>........................] - ETA: 1:43 - loss: 105.3312 - mse: 11233.4004total_loss [105.967896]\n","19/78 [======>.......................] - ETA: 1:41 - loss: 105.3647 - mse: 11233.1797total_loss [102.001198]\n","20/78 [======>.......................] - ETA: 1:40 - loss: 105.1966 - mse: 11191.7324total_loss [102.269592]\n","21/78 [=======>......................] - ETA: 1:39 - loss: 105.0572 - mse: 11156.8438total_loss [102.100464]\n","22/78 [=======>......................] - ETA: 1:37 - loss: 104.9228 - mse: 11123.5557total_loss [107.524994]\n","23/78 [=======>......................] - ETA: 1:36 - loss: 105.0359 - mse: 11142.6016total_loss [120.920082]\n","24/78 [========>.....................] - ETA: 1:34 - loss: 105.6978 - mse: 11287.5625total_loss [117.095329]\n","25/78 [========>.....................] - ETA: 1:33 - loss: 106.1537 - mse: 11384.5127total_loss [110.741158]\n","26/78 [=========>....................] - ETA: 1:31 - loss: 106.3301 - mse: 11418.3232total_loss [100.253304]\n","27/78 [=========>....................] - ETA: 1:29 - loss: 106.1050 - mse: 11367.6709total_loss [104.262161]\n","28/78 [=========>....................] - ETA: 1:28 - loss: 106.0392 - mse: 11349.9189total_loss [109.24968]\n","29/78 [==========>...................] - ETA: 1:26 - loss: 106.1499 - mse: 11370.1113total_loss [94.4546814]\n","30/78 [==========>...................] - ETA: 1:24 - loss: 105.7601 - mse: 11288.4971total_loss [106.187302]\n","31/78 [==========>...................] - ETA: 1:23 - loss: 105.7739 - mse: 11288.0859total_loss [88.0570755]\n","32/78 [===========>..................] - ETA: 1:21 - loss: 105.2202 - mse: 11177.6475total_loss [118.337791]\n","33/78 [===========>..................] - ETA: 1:19 - loss: 105.6177 - mse: 11263.2900total_loss [115.764404]\n","34/78 [============>.................] - ETA: 1:17 - loss: 105.9162 - mse: 11326.1758total_loss [102.666214]\n","35/78 [============>.................] - ETA: 1:16 - loss: 105.8233 - mse: 11303.7236total_loss [125.831696]\n","36/78 [============>.................] - ETA: 1:14 - loss: 106.3791 - mse: 11429.5537total_loss [100.194862]\n","37/78 [=============>................] - ETA: 1:12 - loss: 106.2120 - mse: 11391.9717total_loss [97.2726898]\n","38/78 [=============>................] - ETA: 1:11 - loss: 105.9767 - mse: 11341.1816total_loss [133.01857]\n","39/78 [==============>...............] - ETA: 1:09 - loss: 106.6701 - mse: 11504.0732total_loss [93.9687042]\n","40/78 [==============>...............] - ETA: 1:07 - loss: 106.3526 - mse: 11437.2246total_loss [89.4479065]\n","41/78 [==============>...............] - ETA: 1:05 - loss: 105.9403 - mse: 11353.4121total_loss [105.845543]\n","42/78 [===============>..............] - ETA: 1:04 - loss: 105.9380 - mse: 11349.8379total_loss [117.452812]\n","43/78 [===============>..............] - ETA: 1:02 - loss: 106.2058 - mse: 11406.7061total_loss [103.697815]\n","44/78 [===============>..............] - ETA: 1:00 - loss: 106.1488 - mse: 11391.8545total_loss [114.889824]\n","45/78 [================>.............] - ETA: 58s - loss: 106.3430 - mse: 11432.0273 total_loss [100.062164]\n","46/78 [================>.............] - ETA: 57s - loss: 106.2065 - mse: 11401.1670total_loss [105.430168]\n","47/78 [=================>............] - ETA: 55s - loss: 106.1900 - mse: 11395.0889total_loss [101.199738]\n","48/78 [=================>............] - ETA: 53s - loss: 106.0860 - mse: 11371.0537total_loss [79.3238068]\n","49/78 [=================>............] - ETA: 51s - loss: 105.5398 - mse: 11267.4043total_loss [101.975426]\n","50/78 [==================>...........] - ETA: 50s - loss: 105.4685 - mse: 11250.0361total_loss [115.89711]\n","51/78 [==================>...........] - ETA: 48s - loss: 105.6730 - mse: 11292.8223total_loss [111.017288]\n","52/78 [===================>..........] - ETA: 46s - loss: 105.7758 - mse: 11312.6680total_loss [101.800804]\n","53/78 [===================>..........] - ETA: 44s - loss: 105.7008 - mse: 11294.7568total_loss [104.936951]\n","54/78 [===================>..........] - ETA: 43s - loss: 105.6867 - mse: 11289.5166total_loss [108.685181]\n","55/78 [====================>.........] - ETA: 41s - loss: 105.7412 - mse: 11299.0254total_loss [124.419556]\n","56/78 [====================>.........] - ETA: 39s - loss: 106.0747 - mse: 11373.6895total_loss [103.399841]\n","57/78 [====================>.........] - ETA: 37s - loss: 106.0278 - mse: 11361.7217total_loss [104.658951]\n","58/78 [=====================>........] - ETA: 35s - loss: 106.0042 - mse: 11354.6836total_loss [88.3490753]\n","59/78 [=====================>........] - ETA: 34s - loss: 105.7049 - mse: 11294.5283total_loss [99.723877]\n","60/78 [======================>.......] - ETA: 32s - loss: 105.6053 - mse: 11272.0342total_loss [103.880081]\n","61/78 [======================>.......] - ETA: 30s - loss: 105.5770 - mse: 11264.1494total_loss [98.8039551]\n","62/78 [======================>.......] - ETA: 28s - loss: 105.4677 - mse: 11239.9258total_loss [109.488724]\n","63/78 [=======================>......] - ETA: 26s - loss: 105.5316 - mse: 11251.7959total_loss [85.8765717]\n","64/78 [=======================>......] - ETA: 25s - loss: 105.2244 - mse: 11191.2178total_loss [92.5654755]\n","65/78 [========================>.....] - ETA: 23s - loss: 105.0297 - mse: 11150.8662total_loss [70.294075]\n","66/78 [========================>.....] - ETA: 21s - loss: 104.5034 - mse: 11056.7812total_loss [125.625061]\n","67/78 [========================>.....] - ETA: 19s - loss: 104.8186 - mse: 11127.3018total_loss [112.279793]\n","68/78 [=========================>....] - ETA: 18s - loss: 104.9284 - mse: 11149.0586total_loss [120.355171]\n","69/78 [=========================>....] - ETA: 16s - loss: 105.1519 - mse: 11197.4111total_loss [92.0022125]\n","70/78 [=========================>....] - ETA: 14s - loss: 104.9641 - mse: 11158.3691total_loss [103.344231]\n","71/78 [==========================>...] - ETA: 12s - loss: 104.9413 - mse: 11151.6309total_loss [113.900681]\n","72/78 [==========================>...] - ETA: 10s - loss: 105.0657 - mse: 11176.9336total_loss [91.3322]\n","73/78 [===========================>..] - ETA: 9s - loss: 104.8776 - mse: 11138.0928 total_loss [84.3086472]\n","74/78 [===========================>..] - ETA: 7s - loss: 104.5996 - mse: 11083.6309total_loss [110.347336]\n","75/78 [===========================>..] - ETA: 5s - loss: 104.6762 - mse: 11098.2031total_loss [131.721649]\n","76/78 [============================>.] - ETA: 3s - loss: 105.0321 - mse: 11180.4717total_loss [104.558235]\n","77/78 [============================>.] - ETA: 1s - loss: 105.0260 - mse: 11177.2510total_loss [116.466408]\n","78/78 [==============================] - 141s 2s/step - loss: 105.1726 - mse: 11207.8555\n","Epoch 5/90\n","total_loss [102.373116]\n"," 1/78 [..............................] - ETA: 0s - loss: 102.3731 - mse: 10480.2549total_loss [94.8040237]\n"," 2/78 [..............................] - ETA: 1:08 - loss: 98.5886 - mse: 9734.0293total_loss [113.609879]\n"," 3/78 [>.............................] - ETA: 1:31 - loss: 103.5957 - mse: 10791.7539total_loss [106.620331]\n"," 4/78 [>.............................] - ETA: 1:42 - loss: 104.3518 - mse: 10935.7891total_loss [112.188431]\n"," 5/78 [>.............................] - ETA: 1:46 - loss: 105.9192 - mse: 11265.8799total_loss [92.0707092]\n"," 6/78 [=>............................] - ETA: 1:49 - loss: 103.6111 - mse: 10801.0693total_loss [108.950264]\n"," 7/78 [=>............................] - ETA: 1:51 - loss: 104.3738 - mse: 10953.7969total_loss [99.0869]\n"," 8/78 [==>...........................] - ETA: 1:51 - loss: 103.7130 - mse: 10811.8486total_loss [108.108932]\n"," 9/78 [==>...........................] - ETA: 1:52 - loss: 104.2014 - mse: 10909.1475total_loss [95.404892]\n","10/78 [==>...........................] - ETA: 1:52 - loss: 103.3218 - mse: 10728.4424total_loss [80.453949]\n","11/78 [===>..........................] - ETA: 1:51 - loss: 101.2429 - mse: 10341.5693total_loss [103.659821]\n","12/78 [===>..........................] - ETA: 1:51 - loss: 101.4443 - mse: 10375.2178total_loss [111.792656]\n","13/78 [====>.........................] - ETA: 1:50 - loss: 102.2403 - mse: 10538.4785total_loss [119.83828]\n","14/78 [====>.........................] - ETA: 1:49 - loss: 103.4973 - mse: 10811.5312total_loss [133.479965]\n","15/78 [====>.........................] - ETA: 1:47 - loss: 105.4961 - mse: 11278.5566total_loss [91.763382]\n","16/78 [=====>........................] - ETA: 1:46 - loss: 104.6378 - mse: 11099.9287total_loss [122.56665]\n","17/78 [=====>........................] - ETA: 1:45 - loss: 105.6925 - mse: 11330.6729total_loss [99.5675583]\n","18/78 [=====>........................] - ETA: 1:44 - loss: 105.3522 - mse: 11251.9521total_loss [108.780334]\n","19/78 [======>.......................] - ETA: 1:42 - loss: 105.5326 - mse: 11282.5420total_loss [92.2229767]\n","20/78 [======>.......................] - ETA: 1:41 - loss: 104.8671 - mse: 11143.6689total_loss [117.492249]\n","21/78 [=======>......................] - ETA: 1:39 - loss: 105.4683 - mse: 11270.3711total_loss [107.724091]\n","22/78 [=======>......................] - ETA: 1:38 - loss: 105.5709 - mse: 11285.5586total_loss [90.3022919]\n","23/78 [=======>......................] - ETA: 1:36 - loss: 104.9070 - mse: 11149.4248total_loss [131.078186]\n","24/78 [========>.....................] - ETA: 1:34 - loss: 105.9975 - mse: 11400.7617total_loss [96.8057175]\n","25/78 [========>.....................] - ETA: 1:33 - loss: 105.6298 - mse: 11319.5850total_loss [85.1811218]\n","26/78 [=========>....................] - ETA: 1:31 - loss: 104.8433 - mse: 11163.2861total_loss [108.293961]\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-bd1b62fe0ebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myolov2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_train_data_by_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpascal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-3ba12a2aaf35>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(self, generator, data_size, callbacks)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mlr_schedule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmodel_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"s3FB_ZM_PPkz"},"source":["imname, input_image, gt_boxes_input, response_anchor_input = pascal.get_by_size(32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vKMkj63888J"},"source":["yolov2.model.predict(input_image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fL4RhzYFPaaB"},"source":["yolov2.train([input_image, gt_boxes_input, response_anchor_input], np.zeros(len(input_image)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTShgcXhcBTS","executionInfo":{"status":"ok","timestamp":1607870636699,"user_tz":-480,"elapsed":863,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}},"outputId":"a779d1df-b478-4620-950c-d23b15353275"},"source":["yolov2.model_summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv1 (Conv2D)                  (None, 416, 416, 32) 864         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","bn1 (BatchNormalization)        (None, 416, 416, 32) 128         conv1[0][0]                      \n","__________________________________________________________________________________________________\n","leaky_re_lu (LeakyReLU)         (None, 416, 416, 32) 0           bn1[0][0]                        \n","__________________________________________________________________________________________________\n","max_pooling2d (MaxPooling2D)    (None, 208, 208, 32) 0           leaky_re_lu[0][0]                \n","__________________________________________________________________________________________________\n","conv2 (Conv2D)                  (None, 208, 208, 64) 18432       max_pooling2d[0][0]              \n","__________________________________________________________________________________________________\n","bn2 (BatchNormalization)        (None, 208, 208, 64) 256         conv2[0][0]                      \n","__________________________________________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)       (None, 208, 208, 64) 0           bn2[0][0]                        \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 104, 104, 64) 0           leaky_re_lu_1[0][0]              \n","__________________________________________________________________________________________________\n","conv3 (Conv2D)                  (None, 104, 104, 128 73728       max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","bn3 (BatchNormalization)        (None, 104, 104, 128 512         conv3[0][0]                      \n","__________________________________________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)       (None, 104, 104, 128 0           bn3[0][0]                        \n","__________________________________________________________________________________________________\n","conv4 (Conv2D)                  (None, 104, 104, 64) 8192        leaky_re_lu_2[0][0]              \n","__________________________________________________________________________________________________\n","bn4 (BatchNormalization)        (None, 104, 104, 64) 256         conv4[0][0]                      \n","__________________________________________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)       (None, 104, 104, 64) 0           bn4[0][0]                        \n","__________________________________________________________________________________________________\n","conv5 (Conv2D)                  (None, 104, 104, 128 73728       leaky_re_lu_3[0][0]              \n","__________________________________________________________________________________________________\n","bn5 (BatchNormalization)        (None, 104, 104, 128 512         conv5[0][0]                      \n","__________________________________________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)       (None, 104, 104, 128 0           bn5[0][0]                        \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 52, 52, 128)  0           leaky_re_lu_4[0][0]              \n","__________________________________________________________________________________________________\n","conv6 (Conv2D)                  (None, 52, 52, 256)  294912      max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","bn6 (BatchNormalization)        (None, 52, 52, 256)  1024        conv6[0][0]                      \n","__________________________________________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)       (None, 52, 52, 256)  0           bn6[0][0]                        \n","__________________________________________________________________________________________________\n","conv7 (Conv2D)                  (None, 52, 52, 128)  32768       leaky_re_lu_5[0][0]              \n","__________________________________________________________________________________________________\n","bn7 (BatchNormalization)        (None, 52, 52, 128)  512         conv7[0][0]                      \n","__________________________________________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)       (None, 52, 52, 128)  0           bn7[0][0]                        \n","__________________________________________________________________________________________________\n","conv8 (Conv2D)                  (None, 52, 52, 256)  294912      leaky_re_lu_6[0][0]              \n","__________________________________________________________________________________________________\n","bn8 (BatchNormalization)        (None, 52, 52, 256)  1024        conv8[0][0]                      \n","__________________________________________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)       (None, 52, 52, 256)  0           bn8[0][0]                        \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 26, 26, 256)  0           leaky_re_lu_7[0][0]              \n","__________________________________________________________________________________________________\n","conv9 (Conv2D)                  (None, 26, 26, 512)  1179648     max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","bn9 (BatchNormalization)        (None, 26, 26, 512)  2048        conv9[0][0]                      \n","__________________________________________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)       (None, 26, 26, 512)  0           bn9[0][0]                        \n","__________________________________________________________________________________________________\n","conv10 (Conv2D)                 (None, 26, 26, 256)  131072      leaky_re_lu_8[0][0]              \n","__________________________________________________________________________________________________\n","bn10 (BatchNormalization)       (None, 26, 26, 256)  1024        conv10[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 256)  0           bn10[0][0]                       \n","__________________________________________________________________________________________________\n","conv11 (Conv2D)                 (None, 26, 26, 512)  1179648     leaky_re_lu_9[0][0]              \n","__________________________________________________________________________________________________\n","bn11 (BatchNormalization)       (None, 26, 26, 512)  2048        conv11[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 512)  0           bn11[0][0]                       \n","__________________________________________________________________________________________________\n","conv12 (Conv2D)                 (None, 26, 26, 256)  131072      leaky_re_lu_10[0][0]             \n","__________________________________________________________________________________________________\n","bn12 (BatchNormalization)       (None, 26, 26, 256)  1024        conv12[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 256)  0           bn12[0][0]                       \n","__________________________________________________________________________________________________\n","conv13 (Conv2D)                 (None, 26, 26, 512)  1179648     leaky_re_lu_11[0][0]             \n","__________________________________________________________________________________________________\n","bn13 (BatchNormalization)       (None, 26, 26, 512)  2048        conv13[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 512)  0           bn13[0][0]                       \n","__________________________________________________________________________________________________\n","max_pooling2d_4 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_12[0][0]             \n","__________________________________________________________________________________________________\n","conv14 (Conv2D)                 (None, 13, 13, 1024) 4718592     max_pooling2d_4[0][0]            \n","__________________________________________________________________________________________________\n","bn14 (BatchNormalization)       (None, 13, 13, 1024) 4096        conv14[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_13 (LeakyReLU)      (None, 13, 13, 1024) 0           bn14[0][0]                       \n","__________________________________________________________________________________________________\n","conv15 (Conv2D)                 (None, 13, 13, 512)  524288      leaky_re_lu_13[0][0]             \n","__________________________________________________________________________________________________\n","bn15 (BatchNormalization)       (None, 13, 13, 512)  2048        conv15[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_14 (LeakyReLU)      (None, 13, 13, 512)  0           bn15[0][0]                       \n","__________________________________________________________________________________________________\n","conv16 (Conv2D)                 (None, 13, 13, 1024) 4718592     leaky_re_lu_14[0][0]             \n","__________________________________________________________________________________________________\n","bn16 (BatchNormalization)       (None, 13, 13, 1024) 4096        conv16[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_15 (LeakyReLU)      (None, 13, 13, 1024) 0           bn16[0][0]                       \n","__________________________________________________________________________________________________\n","conv17 (Conv2D)                 (None, 13, 13, 512)  524288      leaky_re_lu_15[0][0]             \n","__________________________________________________________________________________________________\n","bn17 (BatchNormalization)       (None, 13, 13, 512)  2048        conv17[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_16 (LeakyReLU)      (None, 13, 13, 512)  0           bn17[0][0]                       \n","__________________________________________________________________________________________________\n","conv18 (Conv2D)                 (None, 13, 13, 1024) 4718592     leaky_re_lu_16[0][0]             \n","__________________________________________________________________________________________________\n","bn18 (BatchNormalization)       (None, 13, 13, 1024) 4096        conv18[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_17 (LeakyReLU)      (None, 13, 13, 1024) 0           bn18[0][0]                       \n","__________________________________________________________________________________________________\n","conv19 (Conv2D)                 (None, 13, 13, 1024) 9437184     leaky_re_lu_17[0][0]             \n","__________________________________________________________________________________________________\n","bn19 (BatchNormalization)       (None, 13, 13, 1024) 4096        conv19[0][0]                     \n","__________________________________________________________________________________________________\n","pass_through_conv (Conv2D)      (None, 26, 26, 64)   294912      leaky_re_lu_12[0][0]             \n","__________________________________________________________________________________________________\n","leaky_re_lu_18 (LeakyReLU)      (None, 13, 13, 1024) 0           bn19[0][0]                       \n","__________________________________________________________________________________________________\n","pass_through_bn (BatchNormaliza (None, 26, 26, 64)   256         pass_through_conv[0][0]          \n","__________________________________________________________________________________________________\n","conv20 (Conv2D)                 (None, 13, 13, 1024) 9437184     leaky_re_lu_18[0][0]             \n","__________________________________________________________________________________________________\n","leaky_re_lu_20 (LeakyReLU)      (None, 26, 26, 64)   0           pass_through_bn[0][0]            \n","__________________________________________________________________________________________________\n","bn20 (BatchNormalization)       (None, 13, 13, 1024) 4096        conv20[0][0]                     \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 13, 13, 256)  0           leaky_re_lu_20[0][0]             \n","__________________________________________________________________________________________________\n","leaky_re_lu_19 (LeakyReLU)      (None, 13, 13, 1024) 0           bn20[0][0]                       \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 13, 13, 1280) 0           lambda[0][0]                     \n","                                                                 leaky_re_lu_19[0][0]             \n","__________________________________________________________________________________________________\n","conv21 (Conv2D)                 (None, 13, 13, 1024) 11796480    concatenate[0][0]                \n","__________________________________________________________________________________________________\n","bn21 (BatchNormalization)       (None, 13, 13, 1024) 4096        conv21[0][0]                     \n","__________________________________________________________________________________________________\n","leaky_re_lu_21 (LeakyReLU)      (None, 13, 13, 1024) 0           bn21[0][0]                       \n","__________________________________________________________________________________________________\n","conv22 (Conv2D)                 (None, 13, 13, 125)  128000      leaky_re_lu_21[0][0]             \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, None, 5)]    0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            [(None, 13, 13, 5, 1 0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 13, 13, 5, 5 0                                            \n","__________________________________________________________________________________________________\n","yolo_loss (Lambda)              (1,)                 0           conv22[0][0]                     \n","                                                                 input_2[0][0]                    \n","                                                                 input_3[0][0]                    \n","                                                                 input_4[0][0]                    \n","==================================================================================================\n","Total params: 50,938,080\n","Trainable params: 50,917,408\n","Non-trainable params: 20,672\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fX7RgLkWcDnL","executionInfo":{"status":"ok","timestamp":1608214553976,"user_tz":-480,"elapsed":1537,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}}},"source":["yolov2.model_save(\"./drive/My Drive/PapperReproduce/YOLOv2/yolo_v2_model_weights.h5\");"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"eyf5SvHB5Dc3"},"source":["box = np.array([1,2,3,4,5,6,7])/7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-ws9pGo5HRk","executionInfo":{"status":"ok","timestamp":1607825991829,"user_tz":-480,"elapsed":818,"user":{"displayName":"Dongsheng Cheng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQWWC2sT34v1okpTtt7d_H08jDzCcAzd-z85R6=s64","userId":"15430000408402518064"}},"outputId":"2f19e0a4-6b63-4d98-a26e-e9967fbab401"},"source":["box"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.14285714, 0.28571429, 0.42857143, 0.57142857, 0.71428571,\n","       0.85714286, 1.        ])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"ZKVHEEIz5aSR"},"source":[""],"execution_count":null,"outputs":[]}]}